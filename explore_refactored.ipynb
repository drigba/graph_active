{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T \n",
    "\n",
    "from query_strategies import *\n",
    "from augmentation import *\n",
    "from model import *\n",
    "from model_wrapper import *\n",
    "from trainers import *\n",
    "from util import *\n",
    "\n",
    "from hivegraph.contrastive.grace import GRACE\n",
    "from GRACE_new import GRACENew\n",
    "\n",
    "from torch_geometric.seed import seed_everything\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "# generate_balanced_data_splits(dataset,10,\"data_splits\\\\cora_splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT= 0.3\n",
    "NUM_PASSES = 100\n",
    "BUDGET = 150\n",
    "EPOCHS = 100\n",
    "SIGNIFICANCE_ITERATIONS = 10\n",
    "\n",
    "NOISE_PROB = 0.4\n",
    "NOISE_LEVEL = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentations\n",
    "drop_edge = DropEdge(DROPOUT)\n",
    "noise_feature_all = NoiseFeature(NOISE_LEVEL, 1)\n",
    "noise_feature_col = NoiseFeature(NOISE_LEVEL, NOISE_PROB, \"col\")\n",
    "noise_feature_row = NoiseFeature(NOISE_LEVEL, NOISE_PROB, \"row\")\n",
    "noise_latent = NoiseLatent(NOISE_LEVEL)\n",
    "\n",
    "drop_edge_noise_all = T.Compose([drop_edge, noise_feature_all])\n",
    "drop_edge_noise_col = T.Compose([drop_edge, noise_feature_col])\n",
    "drop_edge_noise_row = T.Compose([drop_edge, noise_feature_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategies\n",
    "\n",
    "random_query = RandomQuery()\n",
    "entropy_query = EntropyQuery()\n",
    "\n",
    "augment_sum_entropy = AugmentGraphSumEntropyQuery(drop_edge_noise_all, NUM_PASSES,0.0)\n",
    "augment_logit_change = AugmentGraphLogitChange(drop_edge_noise_all, NUM_PASSES,1.0)\n",
    "augment_latent = AugmentGraphSumQueryLatent(noise_latent, NUM_PASSES)\n",
    "augment_sum_entropy_with_original = AugmentGraphSumEntropyQuery(drop_edge_noise_all, NUM_PASSES, 1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = [torch.load(f\"data_splits/cora_ml/split_{i}.pt\") for i in range(10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GRACE_new2 import GRACENew2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRATEGIES = [ augment_sum_entropy_with_original]\n",
    "# STRATEGIES = noises_latent\n",
    "final_accs = torch.zeros((len(STRATEGIES), BUDGET, SIGNIFICANCE_ITERATIONS))\n",
    "final_auc = torch.zeros((len(STRATEGIES), BUDGET, SIGNIFICANCE_ITERATIONS))\n",
    "\n",
    "# data_splits = [generate_random_data_split(dataset,10,500) for _ in range(SIGNIFICANCE_ITERATIONS)]\n",
    "\n",
    "for strategy_ix, strategy in enumerate(STRATEGIES):\n",
    "    print(f\"Strategy: {strategy}\")\n",
    "    init_wandb(str(strategy), \"SEMI_SUP_DROP_NOISE_TRAIN\", \"CORA\")\n",
    "    data_splits = [torch.load(f\"data_splits/split_{i}.pt\") for i in range(10)]\n",
    "    for dataset in data_splits:\n",
    "        dataset.y_train = dataset.y.clone()\n",
    "    for b in range(1,BUDGET+1):\n",
    "\n",
    "        budget_accuracies = []\n",
    "        budget_aucs = []\n",
    "        for si in range(SIGNIFICANCE_ITERATIONS):\n",
    "            seed_everything(si)\n",
    "            dataset = data_splits[si].to(device)\n",
    "            num_features = dataset.num_features\n",
    "            num_classes = dataset.y.max().item() + 1\n",
    "            \n",
    "            print(f\"{b} - {si} - {strategy}\")\n",
    "            \n",
    "            # model = GCN(num_features,num_classes).to(device)\n",
    "            # loss_fn = F.nll_loss\n",
    "            # optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "            # wrapped_model = ModelWrapper(model,optimizer,loss_fn)\n",
    "            \n",
    "            # model = GRACENew(num_features=data_splits[0].num_features,hidden=128, num_layers=2,\n",
    "            #      drop_edge_rate_1=0.3,drop_edge_rate_2=0.3,\n",
    "            #      drop_feature_rate_1=0.3,drop_feature_rate_2=0.3,\n",
    "            #      ratio=0.5, lambda_=1.0).to(device)\n",
    "            \n",
    "            train_augmentor = drop_edge_noise_col\n",
    "\n",
    "            model = GRACENew2(num_features=dataset.num_features,hidden=128, num_layers=2,\n",
    "                     augmentor1=train_augmentor, augmentor2=train_augmentor).to(device)\n",
    "            supervised_model = torch.nn.Sequential(\n",
    "                        torch.nn.Linear(128, 32),\n",
    "                        torch.nn.ReLU(),\n",
    "                        torch.nn.Linear(32, 7)\n",
    "                    ).to(device)\n",
    "            optimizer = torch.optim.Adam(\n",
    "                list(model.parameters()) + list(supervised_model.parameters()), \n",
    "                lr=0.001, \n",
    "                weight_decay=5e-4\n",
    "            )\n",
    "\n",
    "            balancing_weight = 0.5 if b < 15 else 0.95\n",
    "            wrapped_model = SemiSupervisedModelWrapper(supervised_model,model,optimizer, balancing_weight)\n",
    "            \n",
    "            trainer = Trainer()\n",
    "            \n",
    "            trainer.train(wrapped_model,dataset,200)\n",
    "            acc = trainer.test(wrapped_model,dataset)\n",
    "            \n",
    "            budget_accuracies.append(acc)\n",
    "            # budget_aucs.append(auc)\n",
    "            \n",
    "            # dataset = pool_tuning(wrapped_model, dataset)\n",
    "            \n",
    "            query_node_idx = strategy(wrapped_model,dataset,dataset.train_pool)\n",
    "            print(f'\\tQuery node: {query_node_idx}')\n",
    "            \n",
    "            dataset.train_mask[query_node_idx] = True\n",
    "            dataset.train_pool[query_node_idx] = False\n",
    "            print(f\"\\tTrain mask: {dataset.train_mask.sum()}\")\n",
    "            print(f\"\\tTrain pool: {dataset.train_pool.sum()}\")\n",
    "\n",
    "        budget_accuracies = torch.tensor(budget_accuracies)\n",
    "        budget_aucs = torch.tensor(budget_aucs)\n",
    "        final_accs[strategy_ix, b-1, :] = budget_accuracies\n",
    "        # final_auc[strategy_ix, b-1, :] = budget_aucs\n",
    "        m = budget_accuracies.mean()\n",
    "        std = budget_accuracies.std()\n",
    "        wandb.log({\"accuracy_mean\": m.item(), \"step\":b})\n",
    "        wandb.log({\"accuracy_std\": std.item(), \"step\": b})\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =data_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = wrapped_model.self_supervised_model(dataset.x,dataset.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "out = out.detach().cpu().numpy()\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_out = tsne.fit_transform(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(tsne_out[:,0],tsne_out[:,1],c=dataset.y.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_active",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
