{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "# from hivegraph.contrastive.grace import GRACE\n",
    "from GRACE_new import GRACENew\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from util import * \n",
    "from torch_geometric.utils import to_dense_adj\n",
    "import torch_geometric.transforms as T \n",
    "from augmentation import *\n",
    "from query_strategies import *\n",
    "import matplotlib.pyplot as plt\n",
    "from model import *\n",
    "from model_wrapper import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import wandb\n",
    "from trainers import *\n",
    "from torch_geometric import seed_everything\n",
    "\n",
    "from GRACE_new2 import GRACENew2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT= 0.3\n",
    "NUM_PASSES = 10\n",
    "BUDGET = 150\n",
    "EPOCHS = 100\n",
    "SIGNIFICANCE_ITERATIONS = 10\n",
    "\n",
    "NOISE_PROB = 0.4\n",
    "NOISE_LEVEL = 0.5\n",
    "\n",
    "# Augmentations\n",
    "drop_edge = DropEdge(DROPOUT)\n",
    "noise_feature_all = NoiseFeature(NOISE_LEVEL, 1)\n",
    "noise_feature_col = NoiseFeature(NOISE_LEVEL, NOISE_PROB, \"col\")\n",
    "noise_feature_row = NoiseFeature(NOISE_LEVEL, NOISE_PROB, \"row\")\n",
    "noise_latent = NoiseLatent(NOISE_LEVEL)\n",
    "mask_feature = MaskFeature(DROPOUT)\n",
    "\n",
    "drop_edge_noise_all = T.Compose([drop_edge, noise_feature_all])\n",
    "drop_edge_noise_col = T.Compose([drop_edge, noise_feature_col])\n",
    "drop_edge_noise_row = T.Compose([drop_edge, noise_feature_row])\n",
    "drop_edge_mask_feature = T.Compose([drop_edge, mask_feature])\n",
    "\n",
    "drop_mask_noise = T.Compose([drop_edge, noise_feature_all, mask_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = [torch.load(f\"data_splits/split_{i}.pt\") for i in range(10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "SIGNIFICANCE_ITERATIONS = 10\n",
    "for i in range(0,SIGNIFICANCE_ITERATIONS):\n",
    "    print(i)\n",
    "    seed_everything(i)\n",
    "\n",
    "    dataset_o = data_splits[i].to('cuda')\n",
    "    dataset_o.contrastive_validation_mask = dataset_o.val_mask\n",
    "    dataset_o.contrastive_train_mask = ~dataset_o.val_mask\n",
    "\n",
    "    # model = GRACENew(num_features=dataset_o.num_features,hidden=128, num_layers=2, drop_edge_rate_1=0.3,drop_edge_rate_2=0.3,drop_feature_rate_1=0.3,drop_feature_rate_2=0.3).to(device)\n",
    "    train_augmentor = drop_edge_noise_row\n",
    "    # train_augmentor = noise_feature_all\n",
    "    model = GRACENew2(num_features=dataset_o.num_features,hidden=128, num_layers=2,\n",
    "                     augmentor1=train_augmentor, augmentor2=train_augmentor).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # labels = label_indices(dataset_o)\n",
    "    for epoch in range(200):\n",
    "        opt.zero_grad()\n",
    "        loss = model.train_step(dataset_o)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "\n",
    "for i in range(0,SIGNIFICANCE_ITERATIONS):\n",
    "    dataset_o = data_splits[i].to('cuda')\n",
    "    out = model(dataset_o.x,dataset_o.edge_index).detach().cpu().numpy()\n",
    "    train_mask = dataset_o.train_mask.cpu().numpy()\n",
    "    cc = init_kmeans(dataset_o,dataset_o.train_mask, out)\n",
    "    kmeans = KMeans(n_clusters=7,init=cc,max_iter=500,tol=1e-4,random_state=0)\n",
    "    # kmeans = KMeans(n_clusters=7,init=\"k-means++\",n_init=10,max_iter=500,tol=1e-4,random_state=0)\n",
    "    kmeans.fit(out)\n",
    "    labels = kmeans.labels_\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    label_bin = LabelBinarizer()\n",
    "    label_bin.fit(labels)\n",
    "    labels_bin = label_bin.transform(labels)\n",
    "    distances = kmeans.transform(out)\n",
    "    \n",
    "\n",
    "    val_mask = get_mask_indices( dataset_o.val_mask).cpu().numpy()[:40]\n",
    "    \n",
    "    test_mask = dataset_o.test_mask.cpu().numpy()\n",
    "    features = np.hstack([out,labels_bin,distances])\n",
    "    lr.fit(features[train_mask],dataset_o.y[train_mask].cpu().numpy())\n",
    "    acc = (lr.predict(features)[test_mask] == dataset_o.y.cpu().numpy()[test_mask]).mean()\n",
    "\n",
    "    # mapping = map_labels(dataset_o.y.cpu().numpy()[train_mask],labels[train_mask])[0]\n",
    "    # mapping = map_labels(dataset_o.y.cpu().numpy(),labels)[0]\n",
    "    # mapped_labels = np.vectorize(mapping.get)(labels)\n",
    "    # acc = (mapped_labels == dataset_o.y.cpu().numpy())[test_mask].mean()\n",
    "    \n",
    "    accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31499261447562776"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lr.predict(features) == dataset_o.y.cpu().numpy()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.676, 0.642, 0.599, 0.34, 0.472, 0.631, 0.645, 0.424, 0.673, 0.302]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5404, 0.13568286553577796, 0.302, 0.676)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs = np.array(accs)\n",
    "accs.mean(), accs.std(), accs.min(), accs.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(dataset_o.x,dataset_o.edge_index).detach().cpu().numpy()\n",
    "cc = init_kmeans(dataset_o,dataset_o.train_mask, out)\n",
    "kmeans = KMeans(n_clusters=7,init=cc,max_iter=500,tol=1e-4,random_state=0)\n",
    "# kmeans = KMeans(n_clusters=7,init=\"k-means++\",n_init=10,max_iter=500,tol=1e-4,random_state=0)\n",
    "kmeans.fit(out)\n",
    "\n",
    "mapping, acc = map_labels(dataset_o.y.cpu().numpy(),kmeans.labels_)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tmp = np.vstack([out,kmeans.cluster_centers_])\n",
    "y_tmp = np.hstack([dataset_o.y.cpu().numpy(),np.repeat(-3,7)])\n",
    "tsne = TSNE(n_components=2,random_state=0)\n",
    "tsne_out_c = tsne.fit_transform(out_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask_tmp = np.hstack([train_mask,np.repeat(False,7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter1 = plt.scatter(tsne_out_c[:-7,0],tsne_out_c[:-7,1],c=y_tmp[:-7], cmap=\"tab10\")\n",
    "plt.scatter(tsne_out_c[-7:,0],tsne_out_c[-7:,1],c='green')\n",
    "\n",
    "for i, p in enumerate(tsne_out_c[-7:]):\n",
    "    plt.annotate(mapping.get(i), (p[0], p[1]), fontsize=20)\n",
    "    \n",
    "# train_samples = tsne_out_c[train_mask_tmp]\n",
    "# plt.scatter(train_samples[:,0],train_samples[:,1],c='black')\n",
    "# for i, txt in enumerate(y_tmp[train_mask_tmp]):\n",
    "#     plt.annotate(txt, (train_samples[i, 0], train_samples[i, 1]), fontsize=20)\n",
    "plt.legend(*scatter1.legend_elements(), title=\"Classes\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pred =lr.predict(features)\n",
    "\n",
    "scatter1 = plt.scatter(tsne_out_c[:-7,0],tsne_out_c[:-7,1],c=lr_pred, cmap=\"tab10\")\n",
    "\n",
    "# plt.scatter(tsne_out_c[-7:,0],tsne_out_c[-7:,1],c='green')\n",
    "# for i, p in enumerate(tsne_out_c[-7:]):\n",
    "#     plt.annotate(mapping.get(i), (p[0], p[1]), fontsize=20)\n",
    "    \n",
    "train_samples = tsne_out_c[train_mask_tmp]\n",
    "plt.scatter(train_samples[:,0],train_samples[:,1],c='black')\n",
    "for i, txt in enumerate(y_tmp[train_mask_tmp]):\n",
    "    plt.annotate(txt, (train_samples[i, 0], train_samples[i, 1]), fontsize=20)\n",
    "plt.legend(*scatter1.legend_elements(), title=\"Classes\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = [mapping.get(i) for i in kmeans.labels_]\n",
    "\n",
    "scatter1 = plt.scatter(tsne_out_c[:-7,0],tsne_out_c[:-7,1],c=y_preds, cmap=\"tab10\")\n",
    "plt.scatter(tsne_out_c[-7:,0],tsne_out_c[-7:,1],c='green')\n",
    "\n",
    "for i, p in enumerate(tsne_out_c[-7:]):\n",
    "    plt.annotate(mapping.get(i), (p[0], p[1]), fontsize=20)\n",
    "    \n",
    "train_samples = tsne_out_c[train_mask_tmp]\n",
    "plt.scatter(train_samples[:,0],train_samples[:,1],c='black')\n",
    "# for i, txt in enumerate(y_tmp[train_mask_tmp]):\n",
    "#     plt.annotate(txt, (train_samples[i, 0], train_samples[i, 1]), fontsize=20)\n",
    "plt.legend(*scatter1.legend_elements(), title=\"Classes\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter1 = plt.scatter(tsne_out_c[:-7,0],tsne_out_c[:-7,1],c=y_preds == y_tmp[:-7])\n",
    "plt.scatter(tsne_out_c[-7:,0],tsne_out_c[-7:,1],c='green')\n",
    "\n",
    "for i, p in enumerate(tsne_out_c[-7:]):\n",
    "    plt.annotate(mapping.get(i), (p[0], p[1]), fontsize=20)\n",
    "    \n",
    "train_samples = tsne_out_c[train_mask_tmp]\n",
    "plt.scatter(train_samples[:,0],train_samples[:,1],c='black')\n",
    "# for i, txt in enumerate(y_tmp[train_mask_tmp]):\n",
    "#     plt.annotate(txt, (train_samples[i, 0], train_samples[i, 1]), fontsize=20)\n",
    "plt.legend(*scatter1.legend_elements(), title=\"Classes\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_out = tsne.fit_transform(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = dataset_o.train_mask.detach().cpu().numpy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = kmeans.labels_\n",
    "labels = [mapping[x] for x in kmeans.labels_]\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axs = axs.flatten()\n",
    "# First plot: current plot\n",
    "scatter1 = axs[0].scatter(tsne_out[:,0], tsne_out[:,1], c=dataset_o.y.cpu().numpy(), cmap = \"tab10\")\n",
    "axs[0].set_title(\"True Labels\")\n",
    "axs[0].legend(*scatter1.legend_elements(), title=\"Classes\")\n",
    "\n",
    "# Second plot: color with kmeans.labels_\n",
    "scatter2 = axs[1].scatter(tsne_out[:,0], tsne_out[:,1], c=labels, cmap = \"tab10\")\n",
    "axs[1].set_title(\"KMeans Labels\")\n",
    "axs[1].legend(*scatter2.legend_elements(), title=\"Clusters\")\n",
    "\n",
    "# Third plot: color if kmeans.labels_ == true label\n",
    "correct_labels = labels == dataset_o.y.cpu().numpy()\n",
    "scatter3 = axs[2].scatter(tsne_out[:,0], tsne_out[:,1], c=correct_labels)\n",
    "axs[2].set_title(\"KMeans Correct Labels\")\n",
    "axs[2].legend(*scatter3.legend_elements(), title=\"Correct\")\n",
    "\n",
    "scatter4 = axs[3].scatter(tsne_out[:,0][train_mask], tsne_out[:,1][train_mask], c=dataset_o.y.cpu().numpy()[train_mask], cmap = \"tab10\")\n",
    "axs[3].set_title(\"Train nodes\")\n",
    "axs[3].legend(*scatter4.legend_elements(), title=\"Train nodes\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasaet_o = data_splits[-1].to('cuda')\n",
    "true_labels = dataset_o.y.cpu().numpy()\n",
    "val_mask = dataset_o.val_mask.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3567208271787297"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_ = 1.0\n",
    "model = GRACENew(num_features=data_splits[0].num_features,hidden=128, num_layers=2, drop_edge_rate_1=0.3,drop_edge_rate_2=0.3,drop_feature_rate_1=0.3,drop_feature_rate_2=0.3, lambda_=lambda_).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "wrapped_model = GRACEModelWrapperCluster(model,optimizer)\n",
    "\n",
    "trainer = Trainer()\n",
    "trainer.train(wrapped_model, dataset_o, 200)\n",
    "wrapped_model.test_step(dataset_o)\n",
    "(wrapped_model.cluster_labels_.argmax(axis=1) == dataset_o.y.cpu().numpy()).sum() / dataset_o.y.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_w = wrapped_model(dataset_o).detach().cpu().numpy()\n",
    "tsne = TSNE(n_components=2,random_state=0)\n",
    "tsne_wrapped = tsne.fit_transform(out_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_kmeans_lr = []\n",
    "acc_kmeans_lr_wrap = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tmp = dataset_o.clone()\n",
    "wrapped_model.eval()\n",
    "# init_wandb(\"LATENT_KMEANS\", \"\", \"CORA\")\n",
    "for b in range(1,BUDGET+1):\n",
    "    # KMEANS\n",
    "    train_mask = dataset_tmp.train_mask.cpu().numpy()\n",
    "    cc = init_kmeans(dataset_tmp,dataset_tmp.train_mask, out)\n",
    "    kmeans = KMeans(n_clusters=7,init=cc,max_iter=500,tol=1e-4,random_state=0)\n",
    "    # kmeans = KMeans(n_clusters=7,init=\"k-means++\",n_init=10,max_iter=500,tol=1e-4,random_state=0)\n",
    "    kmeans.fit(out)\n",
    "    kmeans.labels_\n",
    "    pred_labels = kmeans.labels_\n",
    "\n",
    "    best_mapping, best_accuracy = map_labels(true_labels, pred_labels)\n",
    "    print(f\"Best Mapping: {best_mapping}\")\n",
    "    print(f\"Best Accuracy: {best_accuracy}\")\n",
    "\n",
    "    # LOGISTIC REGRESSION\n",
    "    # lb = LabelBinarizer()\n",
    "    # lb.fit(kmeans.labels_)\n",
    "    # cluster_labels =  lb.transform(kmeans.labels_)\n",
    "    # distances = kmeans.transform(out)\n",
    "\n",
    "    # # features = np.hstack([out,distances])\n",
    "    # # features = out\n",
    "    # features = np.hstack([out,distances, cluster_labels])\n",
    "    # lr = LogisticRegression(max_iter=1000, random_state=0)\n",
    "    # lr.fit(features[train_mask],true_labels[train_mask])\n",
    "    # acc = lr.score(features,true_labels )\n",
    "    \n",
    "    # pred_log_probas = lr.predict_log_proba(features)\n",
    "    pred_log_probas = wrapped_model.test_step(dataset_tmp).detach().cpu().numpy()\n",
    "    # acc = pred_log_probas.argmax(dim=1).eq(dataset_tmp.y).sum().item() / dataset_tmp.y.size(0)\n",
    "    acc = torch.argmax(torch.tensor(pred_log_probas), dim=1).eq(dataset_tmp.y.cpu()).sum().item() / dataset_tmp.y.cpu().size(0)\n",
    "    entropies = calculate_entropy(torch.tensor(pred_log_probas))\n",
    "    \n",
    "    # # TTA\n",
    "    for i in range(NUM_PASSES):\n",
    "        data_tmp = dataset_tmp.clone()\n",
    "        data_tmp = drop_edge_noise_all(data_tmp)\n",
    "        # out_c = model(data_tmp.x, data_tmp.edge_index).detach().cpu().numpy()\n",
    "        # # distances = kmeans.transform(out_c)\n",
    "        # # cluster_labels =  lb.transform(kmeans.predict(out_c))\n",
    "        # features = np.hstack([out_c,distances,cluster_labels])\n",
    "        # pred_log_probas = lr.predict_log_proba(features)\n",
    "        pred_log_probas = wrapped_model.test_step(data_tmp).detach().cpu().numpy()\n",
    "        entropies = entropies + calculate_entropy(torch.tensor(pred_log_probas))/NUM_PASSES\n",
    "    \n",
    "    pool_indices = get_mask_indices(dataset_tmp.train_pool).cpu()\n",
    "    chosen_node_ix = torch.argmax(entropies[pool_indices])\n",
    "    chosen_node = pool_indices[chosen_node_ix]\n",
    "    dataset_tmp.train_pool[chosen_node] = False\n",
    "    dataset_tmp.train_mask[chosen_node] = True\n",
    "    predictor = LogisticRegression()\n",
    "    acc_kmeans_lr_wrap.append(acc)\n",
    "    print(f\"Budget {b} - Accuracy: {acc}\")\n",
    "    # wandb.log({\"accuracy_mean\": acc, \"step\": b})\n",
    "    wrapped_model.reset_predictor()\n",
    "wandb.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(acc_kmeans_lr, label=\"LR\")\n",
    "plt.plot(acc_kmeans_lr_wrap, label=\"GRACE\")\n",
    "plt.legend()\n",
    "# plt.yticks(np.arange(0.6,0.85,0.025))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_labels = np.array([best_mapping[i] for i in pred_labels])\n",
    "label_color = dataset_o.y.cpu().numpy() == mapped_labels\n",
    "# label_color = mapped_labels\n",
    "to_plot = out_2[~label_color]\n",
    "\n",
    "scatter = plt.scatter(to_plot[:,0], to_plot[:,1], c=mapped_labels[~label_color])\n",
    "plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tmp = dataset_o.clone()\n",
    "# data_tmp.edge_index = data_tmp.edge_index[:,3:]\n",
    "data_tmp = drop_edge_mask_feature(data_tmp)\n",
    "print(data_tmp.edge_index.shape)\n",
    "\n",
    "out_c = model(data_tmp.x, data_tmp.edge_index).detach().cpu().numpy()\n",
    "\n",
    "tsne = TSNE(n_components=2,random_state=0)\n",
    "out_c2 = tsne.fit_transform(out_c)\n",
    "\n",
    "plt.scatter(out_c2[:,0], out_c2[:,1], c=dataset_o.y.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = LogisticRegression()\n",
    "train_mask = dataset_o.train_mask.cpu().numpy()\n",
    "test_mask = dataset_o.test_mask.cpu().numpy()\n",
    "y = dataset_o.y.cpu().numpy()\n",
    "predictor.fit(out[train_mask], y[train_mask])\n",
    "print(predictor.score(out[test_mask],y[test_mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(out)\n",
    "y_correct = y_pred == y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(out_2[:,0], out_2[:,1], c=y_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUGMENTED ENTROPY\n",
    "dataset = dataset_o.clone()\n",
    "acc_aug = []\n",
    "for b in range(BUDGET):\n",
    "    # if b % 10 == 0:\n",
    "    #     model = GRACE(num_features=dataset_o.num_features,hidden=128, num_layers=2, drop_edge_rate_1=0.3,drop_edge_rate_2=0.3,drop_feature_rate_1=0.3,drop_feature_rate_2=0.3).to(device)\n",
    "    #     opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    #     labels = label_indices(dataset)\n",
    "    #     for epoch in range(200):\n",
    "    #         opt.zero_grad()\n",
    "    #         loss = model.train_step(dataset.x,dataset.edge_index,labels)\n",
    "    #         loss.backward()\n",
    "    #         opt.step()\n",
    "    #     loss = model.train_step(dataset.x,dataset.edge_index,labels)\n",
    "    #     print(dataset.train_mask.sum())\n",
    "    #     print(loss)\n",
    "        \n",
    "    pool_indices = get_mask_indices(dataset.train_pool).cpu()\n",
    "    out = model(dataset.x,dataset.edge_index)\n",
    "    predictor = LogisticRegression()\n",
    "    predictor.fit(out[dataset.train_mask].detach().cpu().numpy(), dataset.y[dataset.train_mask].detach().cpu().numpy())\n",
    "    acc = predictor.score(out[dataset.test_mask].detach().cpu().numpy(), dataset.y[dataset.test_mask].detach().cpu().numpy())\n",
    "\n",
    "    entropy_sum = torch.zeros(dataset.num_nodes)\n",
    "    \n",
    "    for _ in range(NUM_PASSES):\n",
    "        data_tmp = dataset.clone()\n",
    "        data_tmp = drop_edge_noise_all(data_tmp)\n",
    "        out_c = model(data_tmp.x, data_tmp.edge_index)\n",
    "        pred_log_probas = predictor.predict_log_proba(out_c.detach().cpu().numpy())\n",
    "        entropies = calculate_entropy(torch.tensor(pred_log_probas))\n",
    "        entropy_sum += entropies\n",
    "        \n",
    "    pred_log_probas = predictor.predict_log_proba(out.detach().cpu().numpy())\n",
    "    entropy_o = calculate_entropy(torch.tensor(pred_log_probas))\n",
    "    entropy_sum +=entropy_o\n",
    "    entropy_sum /= NUM_PASSES\n",
    "    \n",
    "    chosen_node_ix = torch.argmax(entropy_sum[pool_indices])\n",
    "    chosen_node = pool_indices[chosen_node_ix]\n",
    "    dataset.train_pool[chosen_node] = False\n",
    "    dataset.train_mask[chosen_node] = True\n",
    "    \n",
    "    predictor = LogisticRegression()\n",
    "    acc_aug.append(acc)\n",
    "    print(f\"Budget {b} - Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_n = out.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_log_proba_sklearn_model(model, x):\n",
    "    out = x @ model.coef_.T + model.intercept_\n",
    "    out = torch.tensor(out)\n",
    "    return torch.nn.functional.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictorWrapper():\n",
    "    def __init__(self, model):\n",
    "        self.weights = torch.tensor(model.coef_.T, dtype=torch.float32).cuda()\n",
    "        self.weights.requires_grad = False\n",
    "        self.bias = torch.tensor(model.intercept_, dtype=torch.float32).cuda()\n",
    "        self.bias.requires_grad = False\n",
    "    def predict_log_proba(self, x):\n",
    "        out =  x @ self.weights + self.bias\n",
    "        return torch.nn.functional.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATENT NOISE\n",
    "\n",
    "dataset = dataset_o.clone()\n",
    "acc_laten_noise_o = []\n",
    "out_a = out.clone()\n",
    "for b in range(BUDGET):\n",
    "    pool_indices = get_mask_indices(dataset.train_pool).cpu()\n",
    "\n",
    "    predictor = LogisticRegression()\n",
    "    predictor.fit(out[dataset.train_mask].detach().cpu().numpy(), dataset.y[dataset.train_mask].detach().cpu().numpy())\n",
    "    acc = predictor.score(out[dataset.test_mask].detach().cpu().numpy(), dataset.y[dataset.test_mask].detach().cpu().numpy())\n",
    "\n",
    "    entropy_sum = torch.zeros(dataset.num_nodes)\n",
    "    \n",
    "    for _ in range(NUM_PASSES):\n",
    "        out_c = out_a + noise_latent(out_a)\n",
    "        pred_log_probas = predictor.predict_log_proba(out_c.detach().cpu().numpy())\n",
    "        entropies = calculate_entropy(torch.tensor(pred_log_probas))\n",
    "        entropy_sum += entropies\n",
    "    entropy_sum /= NUM_PASSES\n",
    "    \n",
    "    chosen_node_ix = torch.argmax(entropy_sum[pool_indices])\n",
    "    chosen_node = pool_indices[chosen_node_ix]\n",
    "    dataset.train_pool[chosen_node] = False\n",
    "    dataset.train_mask[chosen_node] = True\n",
    "    \n",
    "    acc_laten_noise_o.append(acc)\n",
    "    print(f\"Budget {b} - Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTROPY\n",
    "dataset = dataset_o.clone()\n",
    "acc_entropy = []\n",
    "for b in range(BUDGET):\n",
    "    predictor = LogisticRegression()\n",
    "    predictor.fit(out[dataset.train_mask].detach().cpu().numpy(), dataset.y[dataset.train_mask].detach().cpu().numpy())\n",
    "    \n",
    "    acc = predictor.score(out[dataset.test_mask].detach().cpu().numpy(), dataset.y[dataset.test_mask].detach().cpu().numpy())\n",
    "    pred_log_probas = predictor.predict_log_proba(out.detach().cpu().numpy())\n",
    "    entropies = calculate_entropy(torch.tensor(pred_log_probas))\n",
    "    pool_indices = get_mask_indices(dataset.train_pool).cpu()\n",
    "    chosen_node_ix = torch.argmax(entropies[pool_indices])\n",
    "    chosen_node = pool_indices[chosen_node_ix]\n",
    "    dataset.train_pool[chosen_node] = False\n",
    "    dataset.train_mask[chosen_node] = True\n",
    "    predictor = LogisticRegression()\n",
    "    acc_entropy.append(acc)\n",
    "    print(f\"Budget {b} - Accuracy: {acc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATENT DISTANCE\n",
    "BUDGET = 150\n",
    "dataset = dataset_o.clone()\n",
    "acc_latent = []\n",
    "for b in range(BUDGET):\n",
    "    \n",
    "    predictor = LogisticRegression()\n",
    "    predictor.fit(out[dataset.train_mask].detach().cpu().numpy(), dataset.y[dataset.train_mask].detach().cpu().numpy())\n",
    "    acc = predictor.score(out[dataset.test_mask].detach().cpu().numpy(), dataset.y[dataset.test_mask].detach().cpu().numpy())\n",
    "    \n",
    "    pool_indices = get_mask_indices(dataset.train_pool).cpu()\n",
    "\n",
    "    dist_matrix = torch.cdist(out[dataset.train_pool], out[dataset.train_pool])\n",
    "    adj = to_dense_adj(dataset.edge_index)[0][dataset.train_pool][:,dataset.train_pool]\n",
    "    adj = adj.cuda()\n",
    "    dist_matrix = dist_matrix * adj\n",
    "    \n",
    "    \n",
    "    max_dist = dist_matrix.mean(dim=1)\n",
    "    min_max_dist = torch.argmin(max_dist)\n",
    "    chosen_node = pool_indices[min_max_dist]\n",
    "\n",
    "    \n",
    "    dataset.train_pool[chosen_node] = False\n",
    "    dataset.train_mask[chosen_node] = True\n",
    "    # predictor = LogisticRegression()\n",
    "    acc_latent.append(acc)\n",
    "    print(f\"Budget {b} - Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(acc_aug, label=\"Augmented Entropy\")\n",
    "plt.plot(acc_entropy, label=\"Entropy\")\n",
    "# plt.plot(acc_latent, label=\"Latent Distance\")\n",
    "# plt.plot(acc_laten_noise, label=\"Latent Noise ADV\")\n",
    "# plt.plot(acc_laten_noise_o, label=\"Latent Noise\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_laten_noise_o[15:41]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_active",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
