{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\banfi\\anaconda3\\envs\\graph_active\\Lib\\site-packages\\torch_geometric\\typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: [WinError 127] A megadott elj치r치s nem tal치lhat칩\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from hivegraph.contrastive.grace import GRACE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from util import * \n",
    "from torch_geometric.utils import to_dense_adj\n",
    "import torch_geometric.transforms as T \n",
    "from augmentation import *\n",
    "from query_strategies import *\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\banfi\\AppData\\Local\\Temp\\ipykernel_29812\\983164769.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_splits = [torch.load(f\"data_splits\\\\cora_splits\\\\split_{i}.pt\") for i in range(10)]\n"
     ]
    }
   ],
   "source": [
    "data_splits = [torch.load(f\"data_splits\\\\cora_splits\\\\split_{i}.pt\") for i in range(10)]\n",
    "dataset_o = data_splits[0].to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRACE(num_features=dataset_o.num_features,hidden=128, num_layers=2, drop_edge_rate_1=0.3,drop_edge_rate_2=0.3,drop_feature_rate_1=0.3,drop_feature_rate_2=0.3).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.5795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.5495, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.9849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(8.0902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.5890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.5925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.4774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.5562, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.5149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.4225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.4340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.4667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.4871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.3651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.3875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.3520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.3644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.3566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.3730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.3463, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.3262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.3254, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.3201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.3504, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.3153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.3309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.3055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.3134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.3023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2588, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2429, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.2003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1839, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1775, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1599, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1346, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.1055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0974, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0974, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0826, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0574, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0509, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0433, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0368, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(7.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9975, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9977, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9789, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9775, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9527, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9467, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9487, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9489, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9427, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9407, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9443, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9231, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9074, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8961, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.9025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8827, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8535, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8505, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8524, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8547, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8456, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8448, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8428, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8438, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8397, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(6.8371, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    opt.zero_grad()\n",
    "    loss = model.train_step(dataset_o.x,dataset_o.edge_index)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(dataset_o.x,dataset_o.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT= 0.3\n",
    "NUM_PASSES = 10\n",
    "BUDGET = 150\n",
    "EPOCHS = 100\n",
    "SIGNIFICANCE_ITERATIONS = 10\n",
    "\n",
    "NOISE_PROB = 0.4\n",
    "NOISE_LEVEL = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentations\n",
    "drop_edge = DropEdge(DROPOUT)\n",
    "noise_feature_all = NoiseFeature(NOISE_LEVEL, 1)\n",
    "noise_feature_col = NoiseFeature(NOISE_LEVEL, NOISE_PROB, \"col\")\n",
    "noise_feature_row = NoiseFeature(NOISE_LEVEL, NOISE_PROB, \"row\")\n",
    "noise_latent = NoiseLatent(NOISE_LEVEL)\n",
    "mask_feature = MaskFeature(DROPOUT)\n",
    "\n",
    "drop_edge_noise_all = T.Compose([drop_edge, noise_feature_all])\n",
    "drop_edge_noise_col = T.Compose([drop_edge, noise_feature_col])\n",
    "drop_edge_noise_row = T.Compose([drop_edge, noise_feature_row])\n",
    "drop_edge_mask_feature = T.Compose([drop_edge, mask_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUGMENTED ENTROPY\n",
    "dataset = dataset_o.clone()\n",
    "acc_aug = []\n",
    "for b in range(BUDGET):\n",
    "    pool_indices = get_mask_indices(dataset.train_pool).cpu()\n",
    "\n",
    "    predictor = LogisticRegression()\n",
    "    predictor.fit(out[dataset.train_mask].detach().cpu().numpy(), dataset.y[dataset.train_mask].detach().cpu().numpy())\n",
    "    acc = predictor.score(out[dataset.test_mask].detach().cpu().numpy(), dataset.y[dataset.test_mask].detach().cpu().numpy())\n",
    "\n",
    "    entropy_sum = torch.zeros(dataset.num_nodes)\n",
    "    \n",
    "    for _ in range(NUM_PASSES):\n",
    "        data_tmp = dataset.clone()\n",
    "        data_tmp = drop_edge_noise_all(data_tmp)\n",
    "        out_c = model(data_tmp.x, data_tmp.edge_index)\n",
    "        pred_log_probas = predictor.predict_log_proba(out_c.detach().cpu().numpy())\n",
    "        entropies = calculate_entropy(torch.tensor(pred_log_probas))\n",
    "        entropy_sum += entropies\n",
    "    entropy_sum /= NUM_PASSES\n",
    "    \n",
    "    chosen_node_ix = torch.argmax(entropy_sum[pool_indices])\n",
    "    chosen_node = pool_indices[chosen_node_ix]\n",
    "    dataset.train_pool[chosen_node] = False\n",
    "    dataset.train_mask[chosen_node] = True\n",
    "    \n",
    "    predictor = LogisticRegression()\n",
    "    acc_aug.append(acc)\n",
    "    print(f\"Budget {b} - Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_n = out.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleLogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleLogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming input_dim is the number of features and output_dim is the number of classes\n",
    "def train_predictor(x, y, train_mask, test_mask):\n",
    "    input_dim = 128\n",
    "    output_dim = y.max().item() + 1  # Assuming y contains class labels starting from 0\n",
    "\n",
    "    model = SimpleLogisticRegression(input_dim, output_dim).to(device)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output[train_mask], y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    _, pred = model(out_n).max(dim=1)\n",
    "    correct = int(pred[test_mask].eq(y[test_mask]).sum().item())\n",
    "    acc = correct / int(test_mask.sum())\n",
    "    return model, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor, _ = train_predictor(out_n, dataset_o.y, dataset_o.train_mask, dataset_o.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLogisticRegression(\n",
       "  (linear): Linear(in_features=128, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor\n",
    "# FINISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATENT NOISE\n",
    "dataset = dataset_o.clone()\n",
    "acc_laten_noise = []\n",
    "out_a = out.clone()\n",
    "for b in range(BUDGET):\n",
    "    pool_indices = get_mask_indices(dataset.train_pool).cpu()\n",
    "\n",
    "    predictor = LogisticRegression()\n",
    "    predictor.fit(out[dataset.train_mask].detach().cpu().numpy(), dataset.y[dataset.train_mask].detach().cpu().numpy())\n",
    "    acc = predictor.score(out[dataset.test_mask].detach().cpu().numpy(), dataset.y[dataset.test_mask].detach().cpu().numpy())\n",
    "\n",
    "    entropy_sum = torch.zeros(dataset.num_nodes)\n",
    "    \n",
    "    for _ in range(NUM_PASSES):\n",
    "        out_c = out_a + noise_latent(out_a)\n",
    "        pred_log_probas = predictor.predict_log_proba(out_c.detach().cpu().numpy())\n",
    "        entropies = calculate_entropy(torch.tensor(pred_log_probas))\n",
    "        entropy_sum += entropies\n",
    "    entropy_sum /= NUM_PASSES\n",
    "    \n",
    "    chosen_node_ix = torch.argmax(entropy_sum[pool_indices])\n",
    "    chosen_node = pool_indices[chosen_node_ix]\n",
    "    dataset.train_pool[chosen_node] = False\n",
    "    dataset.train_mask[chosen_node] = True\n",
    "    \n",
    "    predictor = LogisticRegression()\n",
    "    acc_laten_noise.append(acc)\n",
    "    print(f\"Budget {b} - Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTROPY\n",
    "dataset = dataset_o.clone()\n",
    "acc_entropy = []\n",
    "for b in range(BUDGET):\n",
    "    predictor = LogisticRegression()\n",
    "    predictor.fit(out[dataset.train_mask].detach().cpu().numpy(), dataset.y[dataset.train_mask].detach().cpu().numpy())\n",
    "    \n",
    "    acc = predictor.score(out[dataset.test_mask].detach().cpu().numpy(), dataset.y[dataset.test_mask].detach().cpu().numpy())\n",
    "    pred_log_probas = predictor.predict_log_proba(out.detach().cpu().numpy())\n",
    "    entropies = calculate_entropy(torch.tensor(pred_log_probas))\n",
    "    pool_indices = get_mask_indices(dataset.train_pool).cpu()\n",
    "    chosen_node_ix = torch.argmax(entropies[pool_indices])\n",
    "    chosen_node = pool_indices[chosen_node_ix]\n",
    "    dataset.train_pool[chosen_node] = False\n",
    "    dataset.train_mask[chosen_node] = True\n",
    "    predictor = LogisticRegression()\n",
    "    acc_entropy.append(acc)\n",
    "    print(f\"Budget {b} - Accuracy: {acc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATENT DISTANCE\n",
    "BUDGET = 150\n",
    "dataset = dataset_o.clone()\n",
    "acc_latent = []\n",
    "for b in range(BUDGET):\n",
    "    \n",
    "    predictor = LogisticRegression()\n",
    "    predictor.fit(out[dataset.train_mask].detach().cpu().numpy(), dataset.y[dataset.train_mask].detach().cpu().numpy())\n",
    "    acc = predictor.score(out[dataset.test_mask].detach().cpu().numpy(), dataset.y[dataset.test_mask].detach().cpu().numpy())\n",
    "    \n",
    "    pool_indices = get_mask_indices(dataset.train_pool).cpu()\n",
    "\n",
    "    dist_matrix = torch.cdist(out[dataset.train_pool], out[dataset.train_pool])\n",
    "    adj = to_dense_adj(dataset.edge_index)[0][dataset.train_pool][:,dataset.train_pool]\n",
    "    adj = adj.cuda()\n",
    "    dist_matrix = dist_matrix * adj\n",
    "    \n",
    "    \n",
    "    max_dist = dist_matrix.mean(dim=1)\n",
    "    min_max_dist = torch.argmin(max_dist)\n",
    "    chosen_node = pool_indices[min_max_dist]\n",
    "\n",
    "    \n",
    "    dataset.train_pool[chosen_node] = False\n",
    "    dataset.train_mask[chosen_node] = True\n",
    "    # predictor = LogisticRegression()\n",
    "    acc_latent.append(acc)\n",
    "    print(f\"Budget {b} - Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y, y_hat):\n",
    "    return torch.mean((y-y_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[1],[2],[3]], dtype=torch.float32)\n",
    "t = F.normalize(t, p=2, dim=0)\n",
    "t.requires_grad = True\n",
    "y = torch.tensor([7,8,9], dtype=torch.float32)\n",
    "sgd = torch.optim.SGD([t], lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(30):\n",
    "    sgd.zero_grad()\n",
    "    y_hat = model(t)\n",
    "    l = loss_fn(y, y_hat)\n",
    "    l.backward()\n",
    "    sgd.step()\n",
    "    print(f\"Loss: {l}, t: {t} w:{model.linear.weight[0].item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(acc_aug, label=\"Augmented Entropy\")\n",
    "plt.plot(acc_entropy, label=\"Entropy\")\n",
    "plt.plot(acc_latent, label=\"Latent Distance\")\n",
    "plt.plot(acc_laten_noise, label=\"Latent Noise\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph_active",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
